## Diary for the Project

&nbsp;

### October 1, 2020

&nbsp;

1. I started off by initialising the GitHub repository. I also added some basic references like the proposal to it.
2. Found a few links for downloading the __entire__ databse. Links are:
    1. <https://dumps.wikimedia.org/> is a dump of all the articles in the database. 
    2. <https://en.wikipedia.org/wiki/Wikipedia:Database_download> is an article on how to download the Wikipedia database. There are APIs and downloadable XML/HTML files.
3. Found Python libraries to scrape articles. There are a bunch of resources to scrape articles from Wikipedia. They are:
    1. <https://en.wikipedia.org/wiki/Wikipedia:Database_download> which uses __Beautiful Soup__(the most popular framework apparently).
    2. <https://towardsdatascience.com/step-by-step-tutorial-web-scraping-wikipedia-with-beautifulsoup-48d7f2dfa52d> is an additional resource on Beautiful Soup. 
4. Discussions about algorithm led to a question on domain restriction.

&nbsp;

#### TOTAL TIME = 2 hours
&nbsp;

### October 5, 2020

&nbsp;

1. Searched the web for existing scripts to find an already existing dataset.
2. Found something called WikiLinkGraphs.
3. Added their submodule but there seems to be an error.
4. Author asked to mail him, will do so and ask how to fix/use it for our application. 

- [ ] Send Mail to the Prof.
- [ ] Check licenses to see if we can use the software.
- [ ] Check whether Wikidump is enough to finish the scraping. 

&nbsp;

#### TOTAL TIME = 2 hours
