## Diary for the Project

&nbsp;

### October 1, 2020

&nbsp;

1. I started off by initialising the GitHub repository. I also added some basic references like the proposal to it.
2. Found a few links for downloading the __entire__ databse. Links are:
    1. <https://dumps.wikimedia.org/> is a dump of all the articles in the database. 
    2. <https://en.wikipedia.org/wiki/Wikipedia:Database_download> is an article on how to download the Wikipedia database. There are APIs and downloadable XML/HTML files.
3. Found Python libraries to scrape articles. There are a bunch of resources to scrape articles from Wikipedia. They are:
    1. <https://en.wikipedia.org/wiki/Wikipedia:Database_download> which uses __Beautiful Soup__(the most popular framework apparently).
    2. <https://towardsdatascience.com/step-by-step-tutorial-web-scraping-wikipedia-with-beautifulsoup-48d7f2dfa52d> is an additional resource on Beautiful Soup. 
4. Discussions about algorithm led to a question on domain restriction.
