

## Diary for the project
&nbsp;

## October 1, 2020
    - Cloned the repository and made a diary md file
    - Read about the BeautifulSoup python library for web scraping
    - Started searching for efficient shortest path algorithms
    - Learnt about the radix heap optimization of djiktra's algorithm
    - Discussing about to calculate shortes paths in run time or pre process

## October 2, 2020
    - Started learning BeautifulSoup and NodeJs
    - Discussed about the Frontend requirements

## October 3, 2020
    - Practiced scraping by using BeautifulSoup
    - Praticed by making a server on NodeJs

## October 7, 2020
    - Started learning ExpressJs and invoked it with NodeJs
    - Finding Data Structures to store the data
    - Searched for the most efficient graph storing database
    - Scraped google hompeage for links and theire attributes

## October 8, 2020
    - Found Cassandra NoSQL database and started reading on it
    - Thinking on to use Sql or NoSql databases
    - Analysis of various operations on them

## October 10, 2020
    - Searched for graph databases to store data as a graph
    - Found Neo4j graph database and started reading
    - Will now learn how to use the database and the required languages and frameworks for them

## October 23, 2020
    - Started the CLI version with Siddhant and started scraping for links
    - Face the problems of recognizing important links
    - To solve the problem removed special characters from the wiki link
    - Then added more filetrs to the wikipedia link
    - Read the structure of the Wikipedia page to understand the links required
    - Made a very basic UI and score  system
    - Discussed wats to optimize the game and ways to store the scraped links

## November 13,2020
    - Read the doucmentation and worked with the python library Wiki dump reader for the backend of the game

## November 15, 2020
    - Added the program to add links to the database
    - Downloaded wikipedia dump files and parsed data from them

## WHAT I LEARNT
    - SQL
    - Web Technologies for both frontend and backend
    - Wikipedia Dumps
    - Python libraries for working with SQL and Wikipedia dumps
    - Working with a Large database 
    - Running a local server
    - Parsing XML files
    - Working with a team efficiently
    - Choosing Policies for the best performance
    - Different types of databases(SQL AND NOSQL)
    - Algorithms for path finding and various optimizations
    - Web Scraping

## CONTRIBUTIONS
    - Made the program to add and serach links in the database
    - Parsed Wikipedia dump files
    - Tested the database
    - Suggested policies for the game
    - Made the program to get all stored links in the database
    - Worked with the front end team and gave input on all the pages
    - Worked on the pages to look more attractive
    - Made the required changes for the webpages as decided by the team
    - Actively participated in team discussions